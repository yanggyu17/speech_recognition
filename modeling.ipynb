{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Ubuntu/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "\n",
    "params = {\n",
    "    'num_classes': 12,\n",
    "\n",
    "    # cnn 파라미터\n",
    "    'use_cnn': False,\n",
    "    'num_filters': [8, 16, 32, 32, 64, 128],\n",
    "    'filter_size': [3, 3, 3, 3, 3, 3],\n",
    "    'cnn_batch_norm' : [True, True, True, True, True, True],\n",
    "    'pool_sizes': [2, 2, 2, 2, 1, 1],\n",
    "    'cnn_dropout_keep_prob': [0, 0.2, 0.3, 0.3, 0.3, 0.3],\n",
    "    \n",
    "    # dense 파라미터\n",
    "    'use_fc': False,\n",
    "    'fc_hidden_units': [1028, 512, 256],\n",
    "    'fc_batch_norm': [True, True, True],\n",
    "    'fc_dropout_keep_prob': [0.4, 0.3, 0.3],\n",
    "\n",
    "    # rnn(lstm) 파라미터\n",
    "    'use_rnn': True,\n",
    "    'rnn_n_hiddens': [1024, 512],\n",
    "    'rnn_dropout_keep_prob': [0.7, 0.7],\n",
    "\n",
    "    # Global Average Pooling\n",
    "    'use_gap': False,\n",
    "\n",
    "    'learning_rate': 0.001,\n",
    "    'activation': tf.nn.relu,\n",
    "    'batch_size': 128,\n",
    "    'epochs': 15,\n",
    "    'height': 128,\n",
    "    'width': 100,\n",
    "    'l2_reg': False,\n",
    "    'lambda': 0.01,\n",
    "    'model_path': './6conv/',\n",
    "    'model_file': '6conv'\n",
    "}\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, params,name):\n",
    "        self.num_classes = params['num_classes']\n",
    "        \n",
    "        self.use_cnn = params['use_cnn']\n",
    "        self.num_filters = params['num_filters']\n",
    "        self.filter_sizes = params['filter_size']\n",
    "        self.cnn_batch_norm  = params['cnn_batch_norm']\n",
    "        self.pool_sizes = params['pool_sizes']\n",
    "        self.cnn_dropout_keep_prob = params['cnn_dropout_keep_prob']\n",
    "        \n",
    "        self.use_fc = params['use_fc']\n",
    "        self.fc_hidden_units = params['fc_hidden_units']\n",
    "        self.fc_batch_norm = params['fc_batch_norm']\n",
    "        self.fc_dropout_keep_prob = params['fc_dropout_keep_prob']\n",
    "        \n",
    "        self.use_rnn = params['use_rnn']\n",
    "        self.rnn_n_hiddens = params['rnn_n_hiddens']\n",
    "        self.rnn_dropout_keep_prob = params['rnn_dropout_keep_prob']\n",
    "        \n",
    "        self.use_gap = params['use_gap']\n",
    "        \n",
    "        self.learning_rate = params['learning_rate']\n",
    "        self.activation = params['activation']\n",
    "        \n",
    "        self.height = params['height']\n",
    "        self.width = params['width']\n",
    "        self.model_path = params['model_path']\n",
    "        \n",
    "        self.idx_convolutional_layers = range(1, len(self.filter_sizes) + 1)\n",
    "        self.idx_fc_layers = range(1, len(self.fc_hidden_units) + 1)\n",
    "        self.idx_rnn_layers = range(1, len(self.rnn_n_hiddens) + 1)\n",
    "        \n",
    "        self.name = name \n",
    "\n",
    "        \n",
    "    def convolutional_layers(self, X, is_training=True, reuse=False):\n",
    "        inputs = X\n",
    "        \n",
    "        for i, num_filter, filter_size, use_bn, pool_size, keep_prob in zip(self.idx_convolutional_layers,\n",
    "                                                                            self.num_filters,\n",
    "                                                                            self.filter_sizes,\n",
    "                                                                            self.cnn_batch_norm,\n",
    "                                                                            self.pool_sizes,\n",
    "                                                                            self.cnn_dropout_keep_prob):            \n",
    "            L = tf.layers.conv2d(inputs,\n",
    "                                 filters=num_filter,\n",
    "                                 kernel_size=filter_size,\n",
    "                                 strides=1,\n",
    "                                 padding='SAME',\n",
    "                                 name='CONV' + str(i),\n",
    "                                 reuse=reuse)\n",
    "            \n",
    "#             print(L)\n",
    "            \n",
    "            if use_bn:\n",
    "                L = tf.layers.batch_normalization(L, training=is_training, name='BN' + str(i), reuse=reuse)\n",
    "                \n",
    "            L = self.activation(L)\n",
    "            \n",
    "#             print(L)\n",
    "            \n",
    "            if keep_prob:\n",
    "                L = tf.layers.dropout(L, keep_prob, training = is_training)\n",
    "                \n",
    "#             print(L)\n",
    "            \n",
    "            if pool_size != 1:\n",
    "                L = tf.layers.max_pooling2d(L, pool_size=pool_size, strides=pool_size, padding='SAME')\n",
    "            \n",
    "#             print(L)\n",
    "            \n",
    "            inputs = L\n",
    "            \n",
    "            \n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    \n",
    "    def fc_layers(self, X, is_training=True, reuse=False):\n",
    "        inputs = X\n",
    "        \n",
    "        for i, units, use_bn, keep_prob in zip(self.idx_fc_layers, \n",
    "                                               self.fc_hidden_units, \n",
    "                                               self.fc_batch_norm, \n",
    "                                               self.fc_dropout_keep_prob):\n",
    "            fc = tf.layers.dense(inputs,\n",
    "                                 units=units,\n",
    "                                 reuse=reuse,\n",
    "                                 name='FC'+str(i))\n",
    "            \n",
    "            if use_bn:\n",
    "                fc = tf.layers.batch_normalization(fc, training=is_training, name='fc_BN' + str(i), reuse=reuse)\n",
    "                \n",
    "            fc = self.activation(fc)\n",
    "            \n",
    "            if keep_prob:\n",
    "                fc = tf.layers.dropout(fc, rate=keep_prob, training=is_training, name='fc_dropout' + str(i))\n",
    "                \n",
    "            inputs = fc \n",
    "            \n",
    "        return inputs\n",
    "  \n",
    "\n",
    "    def rnn_layers(self, inputs, is_training=True, reuse=False):\n",
    "        if is_training:\n",
    "            keep_probs = self.rnn_dropout_keep_prob\n",
    "            \n",
    "        else:\n",
    "            keep_probs = np.ones_like(self.rnn_dropout_keep_prob)\n",
    "            \n",
    "        # single layer\n",
    "        if len(self.idx_rnn_layers) == 1:\n",
    "            cell = tf.nn.rnn_cell.BasicLSTMCell(self.rnn_n_hiddens[0], reuse=reuse)\n",
    "            cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=keep_probs[0])\n",
    "        \n",
    "        # multi layer \n",
    "        else:\n",
    "            cell_list = []\n",
    "            \n",
    "            for i, n_hidden, keep_prob in zip(self.idx_rnn_layers, self.rnn_n_hiddens, keep_probs):\n",
    "                cell_ = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, reuse=reuse)\n",
    "                cell_ = tf.nn.rnn_cell.DropoutWrapper(cell_, output_keep_prob=keep_prob)\n",
    "                cell_list.append(cell_)\n",
    "                \n",
    "            cell = tf.nn.rnn_cell.MultiRNNCell(cell_list)\n",
    "            \n",
    "        # output_shape [batch_size, width(n_step), n_classes]\n",
    "        outputs, states = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "        \n",
    "#         print(outputs.get_shape().as_list())\n",
    "        \n",
    "        outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "        outputs = outputs[-1]\n",
    "        \n",
    "        return outputs\n",
    " \n",
    "\n",
    "    def get_reshaped_cnn_to_rnn(self, inputs):\n",
    "        # [batch, height, width, n_feature map]\n",
    "        shape = inputs.get_shape().as_list() \n",
    "        \n",
    "        # 우리가 얻어야하는 사이즈 [batch, width, height x n_feature map]\n",
    "        inputs = tf.transpose(inputs, [0, 2, 1, 3])\n",
    "        reshaped_inputs = tf.reshape(inputs, [-1, shape[2], shape[1] * shape[3]])\n",
    "        \n",
    "        return reshaped_inputs\n",
    "  \n",
    "\n",
    "    def get_logits(self, X, is_training=True, reuse=False):\n",
    "        with tf.variable_scope(self.name):\n",
    "            L = X\n",
    "            \n",
    "            if self.use_cnn:\n",
    "                L = self.convolutional_layers(L, is_training, reuse)\n",
    "\n",
    "            if self.use_rnn:\n",
    "                reshaped_fp = self.get_reshaped_cnn_to_rnn(L)\n",
    "                L = self.rnn_layers(reshaped_fp, is_training, reuse)\n",
    "\n",
    "            if self.use_gap:\n",
    "                shape = L.get_shape().as_list()\n",
    "                \n",
    "                # 글로벌 풀링 사이즈 (height, width)\n",
    "                pool_size = (shape[1], shape[2])\n",
    "                L = tf.layers.average_pooling2d(L, pool_size=pool_size, strides=1, padding='VALID')\n",
    "                \n",
    "                # 마지막 dense layer를 위한 flatten\n",
    "                L = tf.layers.flatten(L)\n",
    "\n",
    "            if self.use_fc:\n",
    "                if not self.use_gap:\n",
    "                    L = tf.layers.flatten(L)\n",
    "                L = self.fc_layers(L, is_training, reuse)\n",
    "                \n",
    "            output = tf.layers.dense(L, units= self.num_classes, reuse=reuse, name='out')\n",
    "            \n",
    "        return output\n",
    "    \n",
    "\n",
    "def train_parser(serialized_example):\n",
    "    features = {\n",
    "        \"spectrum\": tf.FixedLenFeature([128 * 100], tf.float32),\n",
    "        \"label\": tf.FixedLenFeature([12], tf.int64)\n",
    "    }\n",
    "\n",
    "    parsed_feature = tf.parse_single_example(serialized_example, features)\n",
    "\n",
    "    spec = parsed_feature['spectrum']\n",
    "    label = parsed_feature['label']\n",
    "\n",
    "    return spec, label\n",
    "        \n",
    "    \n",
    "def test_parser(serialized_example):\n",
    "    features = {\n",
    "        \"spectrum\": tf.FixedLenFeature([128 * 100], tf.float32),\n",
    "    }\n",
    "\n",
    "    parsed_feature = tf.parse_single_example(serialized_example, features)\n",
    "\n",
    "    spec = parsed_feature['spectrum']\n",
    "\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Var_model/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0 is illegal; using Var_model/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel_0 instead.\n",
      "<tf.Variable 'model/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1152, 4096) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0 is illegal; using Var_model/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias_0 instead.\n",
      "<tf.Variable 'model/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(4096,) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0 is illegal; using Var_model/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel_0 instead.\n",
      "<tf.Variable 'model/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1536, 2048) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0 is illegal; using Var_model/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias_0 instead.\n",
      "<tf.Variable 'model/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2048,) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/out/kernel:0 is illegal; using Var_model/out/kernel_0 instead.\n",
      "<tf.Variable 'model/out/kernel:0' shape=(512, 12) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/out/bias:0 is illegal; using Var_model/out/bias_0 instead.\n",
      "<tf.Variable 'model/out/bias:0' shape=(12,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "test_data_dir = \"../data/tfrecords/test_final.tfrecord\"\n",
    "train_data_dir = \"../data/tfrecords/train_end.tfrecord\"\n",
    "eval_data_dir = \"../data/tfrecords/eval_end.tfrecord\"\n",
    "\n",
    "train_dataset = tf.data.TFRecordDataset(train_data_dir).map(train_parser)\n",
    "train_dataset = train_dataset.shuffle(700000, seed=1, reshuffle_each_iteration=True)\n",
    "train_dataset = train_dataset.batch(params['batch_size'])\n",
    "\n",
    "eval_dataset = tf.data.TFRecordDataset(eval_data_dir).map(train_parser)\n",
    "eval_dataset = eval_dataset.shuffle(700000, seed=1, reshuffle_each_iteration=True)\n",
    "eval_dataset = eval_dataset.batch(params['batch_size'])\n",
    "\n",
    "test_dataset = tf.data.TFRecordDataset(test_data_dir).map(test_parser)\n",
    "test_dataset = test_dataset.batch(params['batch_size'])\n",
    "\n",
    "train_itr = tf.contrib.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\n",
    "eval_itr = tf.contrib.data.Iterator.from_structure(eval_dataset.output_types, eval_dataset.output_shapes)\n",
    "test_itr = tf.contrib.data.Iterator.from_structure(test_dataset.output_types, test_dataset.output_shapes)\n",
    "\n",
    "spec, label = train_itr.get_next()\n",
    "eval_spec, eval_label = eval_itr.get_next()\n",
    "test_spec = test_itr.get_next()\n",
    "\n",
    "eval_label = tf.reshape(eval_label, [-1, 12])\n",
    "eval_label = tf.cast(eval_label, tf.int64)\n",
    "\n",
    "spec = tf.reshape(spec, [-1, 128, 100, 1])\n",
    "spec = tf.cast(spec, tf.float32)\n",
    "\n",
    "eval_spec = tf.reshape(eval_spec, [-1, 128, 100, 1])\n",
    "eval_spec = tf.cast(eval_spec, tf.float32)\n",
    "\n",
    "test_spec = tf.reshape(test_spec, [-1, 128, 100, 1])\n",
    "test_spec = tf.cast(test_spec, tf.float32)\n",
    "\n",
    "train_init_op = train_itr.make_initializer(train_dataset)\n",
    "eval_init_op = eval_itr.make_initializer(eval_dataset)\n",
    "test_init_op = test_itr.make_initializer(test_dataset)\n",
    "\n",
    "name = 'model'\n",
    "model = Model(params, 'model')\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    X = tf.placeholder(tf.float32, [None, params['height'], params['width'], 1])\n",
    "    Y = tf.placeholder(tf.float32, [None, params['num_classes']])\n",
    "    global_step = tf.Variable(0, trainable = False, name = 'global_step')\n",
    "\n",
    "    logits_train = model.get_logits(X)\n",
    "    \n",
    "    loss = tf.losses.softmax_cross_entropy(Y, logits_train)\n",
    "    \n",
    "    # L2 Regularization\n",
    "    if params['l2_reg']:\n",
    "        tv = tf.trainable_variables()\n",
    "        l2 = tf.reduce_sum([tf.nn.l2_loss(v) for v in tv if 'lstm' in v.name and 'bias' not in v.name])   \n",
    "        loss = loss + (params['lambda'] * l2)\n",
    "        \n",
    "    for v in tf.trainable_variables():\n",
    "        tf.summary.histogram('Var_{}'.format(v.name), v)\n",
    "        print(v)\n",
    "\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=name)            \n",
    "    with tf.control_dependencies(update_ops):    \n",
    "        optimizer = tf.train.AdamOptimizer(params['learning_rate']).minimize(loss, global_step=global_step)\n",
    "        \n",
    "    #eval\n",
    "    logits_eval = model.get_logits(X, is_training=False, reuse=True)\n",
    "    predict_proba_ = tf.nn.softmax(logits_eval)\n",
    "    prediction = tf.argmax(predict_proba_, 1)\n",
    "    accuracy = tf.metrics.accuracy(tf.argmax(Y, 1), prediction)\n",
    "    \n",
    "    #predict\n",
    "    logits_test = model.get_logits(X, is_training=False, reuse=True)\n",
    "    test_predict_proba_ = tf.nn.softmax(logits_test)\n",
    "    test_prediction = tf.argmax(test_predict_proba_, 1)\n",
    "    \n",
    "    # 변수 프린트/ 텐서보드 summary 생성            \n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('accuracy', accuracy[1])\n",
    "    \n",
    "#     for v in tf.trainable_variables():\n",
    "#         tf.summary.histogram('Var_{}'.format(v.name), v)\n",
    "#         print(v)\n",
    "        \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "# 모델 저장\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options))\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(params['model_path'], sess.graph)\n",
    "        \n",
    "for epoch in range(params['epochs']):\n",
    "    sess.run(train_init_op)\n",
    "    acc = []\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            step = sess.run(global_step)\n",
    "            \n",
    "            _spec, _label = sess.run([spec, label])\n",
    "            _, c, _summ = sess.run([optimizer, loss, merged], feed_dict = {X: _spec, Y: _label})\n",
    "            acc_train = sess.run(accuracy, feed_dict = {X: _spec, Y: _label})\n",
    "            \n",
    "            acc.append(acc_train[1])\n",
    "            \n",
    "            writer.add_summary(_summ, step)\n",
    "            \n",
    "            if step % 500 == 0:\n",
    "                print('step: {}, cost: {}'.format(step, c))\n",
    "                \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "            \n",
    "    sess.run(eval_init_op)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            step = sess.run(global_step)\n",
    "            \n",
    "            _spec, _label = sess.run([eval_spec, eval_label])\n",
    "            _, c, _summ = sess.run([optimizer, loss, merged], feed_dict = {X: _spec, Y: _label})\n",
    "            acc_train = sess.run(accuracy, feed_dict = {X: _spec, Y: _label})\n",
    "            \n",
    "            acc.append(acc_train[1])\n",
    "            \n",
    "            writer.add_summary(_summ, step)\n",
    "            \n",
    "            if step % 500 == 0:\n",
    "                print('step: {}, cost: {}'.format(step, c))\n",
    "                \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "    \n",
    "#     while True:\n",
    "#         try:\n",
    "#             _spec, _label = sess.run([eval_spec, eval_label])\n",
    "#             val_acc = sess.run(accuracy, feed_dict = {X: _spec, Y: _label})\n",
    "            \n",
    "#             eval_acc.append(val_acc[1]) \n",
    "#         except tf.errors.OutOfRangeError:\n",
    "#             break\n",
    "            \n",
    "    print('epoch: {}, cost : {}, train_acc: {}'.format(epoch, c, np.mean(acc)))\n",
    "    \n",
    "saver.save(sess, params['model_path'] + params['model_file'] + '.ckpt', global_step=sess.run(global_step))\n",
    "\n",
    "print(\"Model is saved.\")\n",
    "\n",
    "sess.run(test_init_op)\n",
    "\n",
    "test_spec_ = sess.run(test_spec)\n",
    "predict = sess.run(test_prediction, feed_dict={X: test_spec_})\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        test_spec_ = sess.run(test_spec)\n",
    "        predict = np.hstack([predict, sess.run(test_prediction, feed_dict={X: test_spec_})])\n",
    "        \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        break\n",
    "        \n",
    "print(np.bincount(predict))\n",
    "print(len(predict))\n",
    "\n",
    "class_names = ['down', 'go', 'left', 'no', 'off', 'on', 'right', 'silence', 'stop', 'unknown', 'up', 'yes']\n",
    "\n",
    "# audio_path = '../data/test/audio/'\n",
    "\n",
    "# files = os.listdir(audio_path)\n",
    "# files = sorted(files)\n",
    "\n",
    "df = pd.read_csv(\"./sub/sample_submission.csv\")\n",
    "files = df['fname']\n",
    "\n",
    "with open(params['model_path'] + 'sub_' + params['model_file'] + '.csv', 'w') as f:\n",
    "    fieldnames=['fname', 'label']\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    \n",
    "    writer.writeheader()\n",
    "    \n",
    "    for i in range(len(predict)):\n",
    "        writer.writerow({'fname': files[i], 'label': class_names[predict[i]]})\n",
    "        \n",
    "print(\"Submission file is created.\")\n",
    "\n",
    "        \n",
    "# predict proba\n",
    "sess.run(test_init_op)\n",
    "\n",
    "test_spec_ = sess.run(test_spec)\n",
    "predict_proba = sess.run(test_predict_proba_, feed_dict={X: test_spec_})\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        test_spec_ = sess.run(test_spec)\n",
    "        predict_proba = np.vstack([predict_proba, sess.run(test_predict_proba_, feed_dict={X: test_spec_})])\n",
    "        \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        break\n",
    "        \n",
    "predict_proba = np.array(predict_proba)\n",
    "print(predict_proba.shape)\n",
    "\n",
    "pp = pd.DataFrame(predict_proba, index = files)\n",
    "pp.to_csv(params['model_path'] + 'proba_' + params['model_file'] + '.csv', index = False)\n",
    "\n",
    "print(\"Proba file is created.\")\n",
    "        \n",
    "print(\"Finish.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
